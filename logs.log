2024-12-14 16:08:57,886:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-12-14 16:08:57,886:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-12-14 16:08:57,886:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-12-14 16:08:57,886:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-12-14 16:13:00,369:INFO:PyCaret ClassificationExperiment
2024-12-14 16:13:00,369:INFO:Logging name: clf-default-name
2024-12-14 16:13:00,369:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-12-14 16:13:00,369:INFO:version 3.3.2
2024-12-14 16:13:00,369:INFO:Initializing setup()
2024-12-14 16:13:00,369:INFO:self.USI: 8c35
2024-12-14 16:13:00,369:INFO:self._variable_keys: {'_available_plots', 'gpu_n_jobs_param', 'USI', 'y', 'y_test', 'exp_id', 'y_train', 'target_param', 'fold_shuffle_param', 'fold_generator', 'n_jobs_param', 'fold_groups_param', 'html_param', 'log_plots_param', 'X_train', 'idx', 'seed', 'data', 'is_multiclass', '_ml_usecase', 'fix_imbalance', 'gpu_param', 'exp_name_log', 'logging_param', 'X', 'X_test', 'pipeline', 'memory'}
2024-12-14 16:13:00,369:INFO:Checking environment
2024-12-14 16:13:00,369:INFO:python_version: 3.11.8
2024-12-14 16:13:00,370:INFO:python_build: ('tags/v3.11.8:db85d51', 'Feb  6 2024 22:03:32')
2024-12-14 16:13:00,370:INFO:machine: AMD64
2024-12-14 16:13:00,370:INFO:platform: Windows-10-10.0.22631-SP0
2024-12-14 16:13:00,373:INFO:Memory: svmem(total=16890978304, available=4208951296, percent=75.1, used=12682027008, free=4208951296)
2024-12-14 16:13:00,374:INFO:Physical Core: 14
2024-12-14 16:13:00,374:INFO:Logical Core: 20
2024-12-14 16:13:00,374:INFO:Checking libraries
2024-12-14 16:13:00,374:INFO:System:
2024-12-14 16:13:00,374:INFO:    python: 3.11.8 (tags/v3.11.8:db85d51, Feb  6 2024, 22:03:32) [MSC v.1937 64 bit (AMD64)]
2024-12-14 16:13:00,374:INFO:executable: c:\Program Files\Python311\python.exe
2024-12-14 16:13:00,374:INFO:   machine: Windows-10-10.0.22631-SP0
2024-12-14 16:13:00,374:INFO:PyCaret required dependencies:
2024-12-14 16:13:00,507:INFO:                 pip: 24.0
2024-12-14 16:13:00,507:INFO:          setuptools: 65.5.0
2024-12-14 16:13:00,507:INFO:             pycaret: 3.3.2
2024-12-14 16:13:00,507:INFO:             IPython: 8.28.0
2024-12-14 16:13:00,507:INFO:          ipywidgets: 8.1.5
2024-12-14 16:13:00,507:INFO:                tqdm: 4.67.1
2024-12-14 16:13:00,507:INFO:               numpy: 1.26.4
2024-12-14 16:13:00,507:INFO:              pandas: 2.1.4
2024-12-14 16:13:00,507:INFO:              jinja2: 3.1.4
2024-12-14 16:13:00,507:INFO:               scipy: 1.11.4
2024-12-14 16:13:00,507:INFO:              joblib: 1.3.2
2024-12-14 16:13:00,507:INFO:             sklearn: 1.4.2
2024-12-14 16:13:00,508:INFO:                pyod: 2.0.2
2024-12-14 16:13:00,508:INFO:            imblearn: 0.12.4
2024-12-14 16:13:00,508:INFO:   category_encoders: 2.6.4
2024-12-14 16:13:00,508:INFO:            lightgbm: 4.5.0
2024-12-14 16:13:00,508:INFO:               numba: 0.60.0
2024-12-14 16:13:00,508:INFO:            requests: 2.32.3
2024-12-14 16:13:00,508:INFO:          matplotlib: 3.7.5
2024-12-14 16:13:00,508:INFO:          scikitplot: 0.3.7
2024-12-14 16:13:00,508:INFO:         yellowbrick: 1.5
2024-12-14 16:13:00,508:INFO:              plotly: 5.24.1
2024-12-14 16:13:00,508:INFO:    plotly-resampler: Not installed
2024-12-14 16:13:00,508:INFO:             kaleido: 0.2.1
2024-12-14 16:13:00,508:INFO:           schemdraw: 0.15
2024-12-14 16:13:00,508:INFO:         statsmodels: 0.14.4
2024-12-14 16:13:00,508:INFO:              sktime: 0.26.0
2024-12-14 16:13:00,508:INFO:               tbats: 1.1.3
2024-12-14 16:13:00,508:INFO:            pmdarima: 2.0.4
2024-12-14 16:13:00,508:INFO:              psutil: 6.1.0
2024-12-14 16:13:00,508:INFO:          markupsafe: 3.0.2
2024-12-14 16:13:00,508:INFO:             pickle5: Not installed
2024-12-14 16:13:00,508:INFO:         cloudpickle: 3.1.0
2024-12-14 16:13:00,508:INFO:         deprecation: 2.1.0
2024-12-14 16:13:00,508:INFO:              xxhash: 3.5.0
2024-12-14 16:13:00,508:INFO:           wurlitzer: Not installed
2024-12-14 16:13:00,508:INFO:PyCaret optional dependencies:
2024-12-14 16:13:00,516:INFO:                shap: Not installed
2024-12-14 16:13:00,516:INFO:           interpret: Not installed
2024-12-14 16:13:00,516:INFO:                umap: Not installed
2024-12-14 16:13:00,516:INFO:     ydata_profiling: Not installed
2024-12-14 16:13:00,516:INFO:  explainerdashboard: Not installed
2024-12-14 16:13:00,516:INFO:             autoviz: Not installed
2024-12-14 16:13:00,516:INFO:           fairlearn: Not installed
2024-12-14 16:13:00,516:INFO:          deepchecks: Not installed
2024-12-14 16:13:00,516:INFO:             xgboost: Not installed
2024-12-14 16:13:00,516:INFO:            catboost: Not installed
2024-12-14 16:13:00,516:INFO:              kmodes: Not installed
2024-12-14 16:13:00,516:INFO:             mlxtend: Not installed
2024-12-14 16:13:00,516:INFO:       statsforecast: Not installed
2024-12-14 16:13:00,516:INFO:        tune_sklearn: Not installed
2024-12-14 16:13:00,516:INFO:                 ray: Not installed
2024-12-14 16:13:00,516:INFO:            hyperopt: Not installed
2024-12-14 16:13:00,516:INFO:              optuna: Not installed
2024-12-14 16:13:00,516:INFO:               skopt: Not installed
2024-12-14 16:13:00,516:INFO:              mlflow: Not installed
2024-12-14 16:13:00,516:INFO:              gradio: Not installed
2024-12-14 16:13:00,516:INFO:             fastapi: Not installed
2024-12-14 16:13:00,516:INFO:             uvicorn: Not installed
2024-12-14 16:13:00,516:INFO:              m2cgen: Not installed
2024-12-14 16:13:00,516:INFO:           evidently: Not installed
2024-12-14 16:13:00,516:INFO:               fugue: Not installed
2024-12-14 16:13:00,516:INFO:           streamlit: Not installed
2024-12-14 16:13:00,516:INFO:             prophet: Not installed
2024-12-14 16:13:00,516:INFO:None
2024-12-14 16:13:00,516:INFO:Set up data.
2024-12-14 16:13:02,804:INFO:Set up folding strategy.
2024-12-14 16:13:02,804:INFO:Set up train/test split.
2024-12-14 16:13:05,518:INFO:Set up index.
2024-12-14 16:13:05,672:INFO:Assigning column types.
2024-12-14 16:13:07,989:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-12-14 16:13:08,017:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-12-14 16:13:08,020:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-12-14 16:13:08,050:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-12-14 16:13:08,050:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-12-14 16:13:08,078:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-12-14 16:13:08,078:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-12-14 16:13:08,095:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-12-14 16:13:08,095:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-12-14 16:13:08,095:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-12-14 16:13:08,122:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-12-14 16:13:08,137:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-12-14 16:13:08,137:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-12-14 16:13:08,163:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-12-14 16:13:08,179:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-12-14 16:13:08,179:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-12-14 16:13:08,179:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-12-14 16:13:08,222:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-12-14 16:13:08,222:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-12-14 16:13:08,263:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-12-14 16:13:08,263:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-12-14 16:13:08,266:INFO:Preparing preprocessing pipeline...
2024-12-14 16:13:08,590:INFO:Set up simple imputation.
2024-12-14 16:13:08,590:INFO:Set up feature normalization.
2024-12-14 16:13:08,897:INFO:Set up column name cleaning.
2024-12-14 16:13:45,470:INFO:PyCaret ClassificationExperiment
2024-12-14 16:13:45,470:INFO:Logging name: clf-default-name
2024-12-14 16:13:45,470:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-12-14 16:13:45,470:INFO:version 3.3.2
2024-12-14 16:13:45,470:INFO:Initializing setup()
2024-12-14 16:13:45,470:INFO:self.USI: bd8b
2024-12-14 16:13:45,470:INFO:self._variable_keys: {'_available_plots', 'gpu_n_jobs_param', 'USI', 'y', 'y_test', 'exp_id', 'y_train', 'target_param', 'fold_shuffle_param', 'fold_generator', 'n_jobs_param', 'fold_groups_param', 'html_param', 'log_plots_param', 'X_train', 'idx', 'seed', 'data', 'is_multiclass', '_ml_usecase', 'fix_imbalance', 'gpu_param', 'exp_name_log', 'logging_param', 'X', 'X_test', 'pipeline', 'memory'}
2024-12-14 16:13:45,470:INFO:Checking environment
2024-12-14 16:13:45,470:INFO:python_version: 3.11.8
2024-12-14 16:13:45,470:INFO:python_build: ('tags/v3.11.8:db85d51', 'Feb  6 2024 22:03:32')
2024-12-14 16:13:45,470:INFO:machine: AMD64
2024-12-14 16:13:45,470:INFO:platform: Windows-10-10.0.22631-SP0
2024-12-14 16:13:45,473:INFO:Memory: svmem(total=16890978304, available=1413332992, percent=91.6, used=15477645312, free=1413332992)
2024-12-14 16:13:45,473:INFO:Physical Core: 14
2024-12-14 16:13:45,473:INFO:Logical Core: 20
2024-12-14 16:13:45,473:INFO:Checking libraries
2024-12-14 16:13:45,473:INFO:System:
2024-12-14 16:13:45,473:INFO:    python: 3.11.8 (tags/v3.11.8:db85d51, Feb  6 2024, 22:03:32) [MSC v.1937 64 bit (AMD64)]
2024-12-14 16:13:45,473:INFO:executable: c:\Program Files\Python311\python.exe
2024-12-14 16:13:45,473:INFO:   machine: Windows-10-10.0.22631-SP0
2024-12-14 16:13:45,473:INFO:PyCaret required dependencies:
2024-12-14 16:13:45,473:INFO:                 pip: 24.0
2024-12-14 16:13:45,473:INFO:          setuptools: 65.5.0
2024-12-14 16:13:45,473:INFO:             pycaret: 3.3.2
2024-12-14 16:13:45,473:INFO:             IPython: 8.28.0
2024-12-14 16:13:45,473:INFO:          ipywidgets: 8.1.5
2024-12-14 16:13:45,473:INFO:                tqdm: 4.67.1
2024-12-14 16:13:45,473:INFO:               numpy: 1.26.4
2024-12-14 16:13:45,473:INFO:              pandas: 2.1.4
2024-12-14 16:13:45,473:INFO:              jinja2: 3.1.4
2024-12-14 16:13:45,473:INFO:               scipy: 1.11.4
2024-12-14 16:13:45,473:INFO:              joblib: 1.3.2
2024-12-14 16:13:45,473:INFO:             sklearn: 1.4.2
2024-12-14 16:13:45,473:INFO:                pyod: 2.0.2
2024-12-14 16:13:45,473:INFO:            imblearn: 0.12.4
2024-12-14 16:13:45,473:INFO:   category_encoders: 2.6.4
2024-12-14 16:13:45,473:INFO:            lightgbm: 4.5.0
2024-12-14 16:13:45,473:INFO:               numba: 0.60.0
2024-12-14 16:13:45,473:INFO:            requests: 2.32.3
2024-12-14 16:13:45,473:INFO:          matplotlib: 3.7.5
2024-12-14 16:13:45,473:INFO:          scikitplot: 0.3.7
2024-12-14 16:13:45,473:INFO:         yellowbrick: 1.5
2024-12-14 16:13:45,473:INFO:              plotly: 5.24.1
2024-12-14 16:13:45,473:INFO:    plotly-resampler: Not installed
2024-12-14 16:13:45,474:INFO:             kaleido: 0.2.1
2024-12-14 16:13:45,474:INFO:           schemdraw: 0.15
2024-12-14 16:13:45,474:INFO:         statsmodels: 0.14.4
2024-12-14 16:13:45,474:INFO:              sktime: 0.26.0
2024-12-14 16:13:45,474:INFO:               tbats: 1.1.3
2024-12-14 16:13:45,474:INFO:            pmdarima: 2.0.4
2024-12-14 16:13:45,474:INFO:              psutil: 6.1.0
2024-12-14 16:13:45,474:INFO:          markupsafe: 3.0.2
2024-12-14 16:13:45,474:INFO:             pickle5: Not installed
2024-12-14 16:13:45,474:INFO:         cloudpickle: 3.1.0
2024-12-14 16:13:45,474:INFO:         deprecation: 2.1.0
2024-12-14 16:13:45,474:INFO:              xxhash: 3.5.0
2024-12-14 16:13:45,474:INFO:           wurlitzer: Not installed
2024-12-14 16:13:45,474:INFO:PyCaret optional dependencies:
2024-12-14 16:13:45,474:INFO:                shap: Not installed
2024-12-14 16:13:45,474:INFO:           interpret: Not installed
2024-12-14 16:13:45,474:INFO:                umap: Not installed
2024-12-14 16:13:45,474:INFO:     ydata_profiling: Not installed
2024-12-14 16:13:45,474:INFO:  explainerdashboard: Not installed
2024-12-14 16:13:45,474:INFO:             autoviz: Not installed
2024-12-14 16:13:45,474:INFO:           fairlearn: Not installed
2024-12-14 16:13:45,474:INFO:          deepchecks: Not installed
2024-12-14 16:13:45,474:INFO:             xgboost: Not installed
2024-12-14 16:13:45,474:INFO:            catboost: Not installed
2024-12-14 16:13:45,474:INFO:              kmodes: Not installed
2024-12-14 16:13:45,474:INFO:             mlxtend: Not installed
2024-12-14 16:13:45,474:INFO:       statsforecast: Not installed
2024-12-14 16:13:45,474:INFO:        tune_sklearn: Not installed
2024-12-14 16:13:45,474:INFO:                 ray: Not installed
2024-12-14 16:13:45,474:INFO:            hyperopt: Not installed
2024-12-14 16:13:45,474:INFO:              optuna: Not installed
2024-12-14 16:13:45,474:INFO:               skopt: Not installed
2024-12-14 16:13:45,474:INFO:              mlflow: Not installed
2024-12-14 16:13:45,474:INFO:              gradio: Not installed
2024-12-14 16:13:45,474:INFO:             fastapi: Not installed
2024-12-14 16:13:45,474:INFO:             uvicorn: Not installed
2024-12-14 16:13:45,474:INFO:              m2cgen: Not installed
2024-12-14 16:13:45,474:INFO:           evidently: Not installed
2024-12-14 16:13:45,474:INFO:               fugue: Not installed
2024-12-14 16:13:45,474:INFO:           streamlit: Not installed
2024-12-14 16:13:45,474:INFO:             prophet: Not installed
2024-12-14 16:13:45,474:INFO:None
2024-12-14 16:13:45,474:INFO:Set up data.
2024-12-14 16:13:52,308:INFO:Set up folding strategy.
2024-12-14 16:13:52,308:INFO:Set up train/test split.
2024-12-14 16:13:55,072:INFO:Set up index.
2024-12-14 16:13:55,212:INFO:Assigning column types.
2024-12-14 16:13:57,628:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-12-14 16:13:57,652:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-12-14 16:13:57,653:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-12-14 16:13:57,667:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-12-14 16:13:57,668:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-12-14 16:13:57,692:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-12-14 16:13:57,692:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-12-14 16:13:57,707:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-12-14 16:13:57,707:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-12-14 16:13:57,707:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-12-14 16:13:57,732:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-12-14 16:13:57,747:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-12-14 16:13:57,748:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-12-14 16:13:57,772:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-12-14 16:13:57,787:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-12-14 16:13:57,787:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-12-14 16:13:57,787:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-12-14 16:13:57,834:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-12-14 16:13:57,834:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-12-14 16:13:57,880:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-12-14 16:13:57,880:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-12-14 16:13:57,881:INFO:Preparing preprocessing pipeline...
2024-12-14 16:13:58,200:INFO:Set up simple imputation.
2024-12-14 16:13:58,202:INFO:Set up feature normalization.
2024-12-14 16:13:58,509:INFO:Set up column name cleaning.
2024-12-14 16:14:12,053:INFO:Finished creating preprocessing pipeline.
2024-12-14 16:14:12,059:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\lmaos\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Unnamed: 0.1', 'Unnamed: 0',
                                             'uid', 'originh', 'originp',
                                             'responh', 'responp',
                                             'flow_duration', 'fwd_pkts_tot',
                                             'bwd_pkts_tot',
                                             'fwd_data_pkts_tot',
                                             'bwd_data_pkts_tot',
                                             'fwd_pkts_per_sec',
                                             'bwd_pkts_per_sec',
                                             'flow_p...
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-12-14 16:14:12,059:INFO:Creating final display dataframe.
2024-12-14 16:14:29,910:INFO:Setup _display_container:                     Description             Value
0                    Session id                42
1                        Target  traffic_category
2                   Target type        Multiclass
3           Original data shape     (2084586, 88)
4        Transformed data shape     (2084586, 88)
5   Transformed train set shape     (1459210, 88)
6    Transformed test set shape      (625376, 88)
7              Numeric features                87
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                    Normalize              True
13             Normalize method            zscore
14               Fold Generator   StratifiedKFold
15                  Fold Number                10
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              bd8b
2024-12-14 16:14:29,954:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-12-14 16:14:29,954:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-12-14 16:14:29,995:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-12-14 16:14:29,995:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-12-14 16:14:30,008:INFO:setup() successfully completed in 44.54s...............
2024-12-14 16:14:30,020:INFO:Initializing compare_models()
2024-12-14 16:14:30,020:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021997ED08D0>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000021997ED08D0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-12-14 16:14:30,020:INFO:Checking exceptions
2024-12-14 16:14:31,580:INFO:Preparing display monitor
2024-12-14 16:14:31,753:INFO:Initializing Logistic Regression
2024-12-14 16:14:31,753:INFO:Total runtime is 0.0 minutes
2024-12-14 16:14:31,755:INFO:SubProcess create_model() called ==================================
2024-12-14 16:14:31,757:INFO:Initializing create_model()
2024-12-14 16:14:31,757:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021997ED08D0>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000219AB06C890>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-12-14 16:14:31,757:INFO:Checking exceptions
2024-12-14 16:14:31,757:INFO:Importing libraries
2024-12-14 16:14:31,757:INFO:Copying training dataset
2024-12-14 16:14:34,242:INFO:Defining folds
2024-12-14 16:14:34,243:INFO:Declaring metric variables
2024-12-14 16:14:34,245:INFO:Importing untrained model
2024-12-14 16:14:34,248:INFO:Logistic Regression Imported successfully
2024-12-14 16:14:34,253:INFO:Starting cross validation
2024-12-14 16:14:34,256:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-12-14 16:21:24,596:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-12-14 16:21:24,597:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-12-14 16:21:24,597:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-12-14 16:21:24,597:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-12-14 16:22:04,286:INFO:PyCaret ClassificationExperiment
2024-12-14 16:22:04,286:INFO:Logging name: clf-default-name
2024-12-14 16:22:04,286:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-12-14 16:22:04,286:INFO:version 3.3.2
2024-12-14 16:22:04,286:INFO:Initializing setup()
2024-12-14 16:22:04,286:INFO:self.USI: e003
2024-12-14 16:22:04,286:INFO:self._variable_keys: {'memory', 'USI', 'gpu_param', 'X_test', 'exp_id', 'X_train', 'exp_name_log', 'data', 'y_train', 'pipeline', 'target_param', 'seed', '_ml_usecase', 'fold_generator', 'idx', 'is_multiclass', 'fold_groups_param', '_available_plots', 'html_param', 'X', 'fold_shuffle_param', 'logging_param', 'gpu_n_jobs_param', 'n_jobs_param', 'y', 'log_plots_param', 'y_test', 'fix_imbalance'}
2024-12-14 16:22:04,286:INFO:Checking environment
2024-12-14 16:22:04,286:INFO:python_version: 3.11.8
2024-12-14 16:22:04,286:INFO:python_build: ('tags/v3.11.8:db85d51', 'Feb  6 2024 22:03:32')
2024-12-14 16:22:04,286:INFO:machine: AMD64
2024-12-14 16:22:04,286:INFO:platform: Windows-10-10.0.22631-SP0
2024-12-14 16:22:04,288:INFO:Memory: svmem(total=16890978304, available=3260334080, percent=80.7, used=13630644224, free=3260334080)
2024-12-14 16:22:04,288:INFO:Physical Core: 14
2024-12-14 16:22:04,288:INFO:Logical Core: 20
2024-12-14 16:22:04,288:INFO:Checking libraries
2024-12-14 16:22:04,288:INFO:System:
2024-12-14 16:22:04,288:INFO:    python: 3.11.8 (tags/v3.11.8:db85d51, Feb  6 2024, 22:03:32) [MSC v.1937 64 bit (AMD64)]
2024-12-14 16:22:04,288:INFO:executable: c:\Program Files\Python311\python.exe
2024-12-14 16:22:04,288:INFO:   machine: Windows-10-10.0.22631-SP0
2024-12-14 16:22:04,288:INFO:PyCaret required dependencies:
2024-12-14 16:22:04,365:INFO:                 pip: 24.0
2024-12-14 16:22:04,365:INFO:          setuptools: 65.5.0
2024-12-14 16:22:04,365:INFO:             pycaret: 3.3.2
2024-12-14 16:22:04,365:INFO:             IPython: 8.28.0
2024-12-14 16:22:04,365:INFO:          ipywidgets: 8.1.5
2024-12-14 16:22:04,365:INFO:                tqdm: 4.67.1
2024-12-14 16:22:04,365:INFO:               numpy: 1.26.4
2024-12-14 16:22:04,365:INFO:              pandas: 2.1.4
2024-12-14 16:22:04,365:INFO:              jinja2: 3.1.4
2024-12-14 16:22:04,365:INFO:               scipy: 1.11.4
2024-12-14 16:22:04,365:INFO:              joblib: 1.3.2
2024-12-14 16:22:04,365:INFO:             sklearn: 1.4.2
2024-12-14 16:22:04,365:INFO:                pyod: 2.0.2
2024-12-14 16:22:04,365:INFO:            imblearn: 0.12.4
2024-12-14 16:22:04,365:INFO:   category_encoders: 2.6.4
2024-12-14 16:22:04,365:INFO:            lightgbm: 4.5.0
2024-12-14 16:22:04,365:INFO:               numba: 0.60.0
2024-12-14 16:22:04,365:INFO:            requests: 2.32.3
2024-12-14 16:22:04,365:INFO:          matplotlib: 3.7.5
2024-12-14 16:22:04,365:INFO:          scikitplot: 0.3.7
2024-12-14 16:22:04,365:INFO:         yellowbrick: 1.5
2024-12-14 16:22:04,365:INFO:              plotly: 5.24.1
2024-12-14 16:22:04,365:INFO:    plotly-resampler: Not installed
2024-12-14 16:22:04,365:INFO:             kaleido: 0.2.1
2024-12-14 16:22:04,365:INFO:           schemdraw: 0.15
2024-12-14 16:22:04,365:INFO:         statsmodels: 0.14.4
2024-12-14 16:22:04,365:INFO:              sktime: 0.26.0
2024-12-14 16:22:04,365:INFO:               tbats: 1.1.3
2024-12-14 16:22:04,365:INFO:            pmdarima: 2.0.4
2024-12-14 16:22:04,365:INFO:              psutil: 6.1.0
2024-12-14 16:22:04,365:INFO:          markupsafe: 3.0.2
2024-12-14 16:22:04,365:INFO:             pickle5: Not installed
2024-12-14 16:22:04,365:INFO:         cloudpickle: 3.1.0
2024-12-14 16:22:04,365:INFO:         deprecation: 2.1.0
2024-12-14 16:22:04,365:INFO:              xxhash: 3.5.0
2024-12-14 16:22:04,365:INFO:           wurlitzer: Not installed
2024-12-14 16:22:04,365:INFO:PyCaret optional dependencies:
2024-12-14 16:22:04,377:INFO:                shap: Not installed
2024-12-14 16:22:04,377:INFO:           interpret: Not installed
2024-12-14 16:22:04,377:INFO:                umap: Not installed
2024-12-14 16:22:04,377:INFO:     ydata_profiling: Not installed
2024-12-14 16:22:04,377:INFO:  explainerdashboard: Not installed
2024-12-14 16:22:04,377:INFO:             autoviz: Not installed
2024-12-14 16:22:04,377:INFO:           fairlearn: Not installed
2024-12-14 16:22:04,377:INFO:          deepchecks: Not installed
2024-12-14 16:22:04,377:INFO:             xgboost: Not installed
2024-12-14 16:22:04,377:INFO:            catboost: Not installed
2024-12-14 16:22:04,377:INFO:              kmodes: Not installed
2024-12-14 16:22:04,377:INFO:             mlxtend: Not installed
2024-12-14 16:22:04,377:INFO:       statsforecast: Not installed
2024-12-14 16:22:04,377:INFO:        tune_sklearn: Not installed
2024-12-14 16:22:04,377:INFO:                 ray: Not installed
2024-12-14 16:22:04,377:INFO:            hyperopt: Not installed
2024-12-14 16:22:04,377:INFO:              optuna: Not installed
2024-12-14 16:22:04,377:INFO:               skopt: Not installed
2024-12-14 16:22:04,377:INFO:              mlflow: Not installed
2024-12-14 16:22:04,377:INFO:              gradio: Not installed
2024-12-14 16:22:04,377:INFO:             fastapi: Not installed
2024-12-14 16:22:04,377:INFO:             uvicorn: Not installed
2024-12-14 16:22:04,377:INFO:              m2cgen: Not installed
2024-12-14 16:22:04,377:INFO:           evidently: Not installed
2024-12-14 16:22:04,377:INFO:               fugue: Not installed
2024-12-14 16:22:04,377:INFO:           streamlit: Not installed
2024-12-14 16:22:04,377:INFO:             prophet: Not installed
2024-12-14 16:22:04,377:INFO:None
2024-12-14 16:22:04,377:INFO:Set up data.
2024-12-14 16:22:04,724:INFO:Set up folding strategy.
2024-12-14 16:22:04,724:INFO:Set up train/test split.
2024-12-14 16:22:05,206:INFO:Set up index.
2024-12-14 16:22:05,227:INFO:Assigning column types.
2024-12-14 16:22:05,680:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-12-14 16:22:05,704:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-12-14 16:22:05,704:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-12-14 16:22:05,720:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-12-14 16:22:05,720:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-12-14 16:22:05,746:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-12-14 16:22:05,746:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-12-14 16:22:05,766:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-12-14 16:22:05,766:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-12-14 16:22:05,766:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-12-14 16:22:05,787:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-12-14 16:22:05,799:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-12-14 16:22:05,799:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-12-14 16:22:05,830:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-12-14 16:22:05,843:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-12-14 16:22:05,843:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-12-14 16:22:05,843:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-12-14 16:22:05,879:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-12-14 16:22:05,879:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-12-14 16:22:05,911:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-12-14 16:22:05,911:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-12-14 16:22:05,942:INFO:Preparing preprocessing pipeline...
2024-12-14 16:22:06,015:INFO:Set up simple imputation.
2024-12-14 16:22:06,015:INFO:Set up feature normalization.
2024-12-14 16:22:06,080:INFO:Set up column name cleaning.
2024-12-14 16:22:07,940:INFO:Finished creating preprocessing pipeline.
2024-12-14 16:22:07,940:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\lmaos\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Unnamed: 0.1', 'Unnamed: 0',
                                             'uid', 'originh', 'originp',
                                             'responh', 'responp',
                                             'flow_duration', 'fwd_pkts_tot',
                                             'bwd_pkts_tot',
                                             'fwd_data_pkts_tot',
                                             'bwd_data_pkts_tot',
                                             'fwd_pkts_per_sec',
                                             'bwd_pkts_per_sec',
                                             'flow_p...
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-12-14 16:22:07,940:INFO:Creating final display dataframe.
2024-12-14 16:22:10,894:INFO:Setup _display_container:                     Description             Value
0                    Session id                42
1                        Target  traffic_category
2                   Target type        Multiclass
3           Original data shape      (416917, 88)
4        Transformed data shape      (416917, 88)
5   Transformed train set shape      (291841, 88)
6    Transformed test set shape      (125076, 88)
7              Numeric features                87
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                    Normalize              True
13             Normalize method            zscore
14               Fold Generator   StratifiedKFold
15                  Fold Number                10
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              e003
2024-12-14 16:22:10,933:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-12-14 16:22:10,933:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-12-14 16:22:10,979:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-12-14 16:22:10,979:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-12-14 16:22:10,980:INFO:setup() successfully completed in 6.7s...............
2024-12-14 16:22:10,980:INFO:Initializing compare_models()
2024-12-14 16:22:10,980:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002C950029690>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000002C950029690>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-12-14 16:22:10,980:INFO:Checking exceptions
2024-12-14 16:22:11,299:INFO:Preparing display monitor
2024-12-14 16:22:11,331:INFO:Initializing Logistic Regression
2024-12-14 16:22:11,331:INFO:Total runtime is 0.0 minutes
2024-12-14 16:22:11,335:INFO:SubProcess create_model() called ==================================
2024-12-14 16:22:11,339:INFO:Initializing create_model()
2024-12-14 16:22:11,339:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002C950029690>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002C923C9CF10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-12-14 16:22:11,339:INFO:Checking exceptions
2024-12-14 16:22:11,339:INFO:Importing libraries
2024-12-14 16:22:11,339:INFO:Copying training dataset
2024-12-14 16:22:11,819:INFO:Defining folds
2024-12-14 16:22:11,819:INFO:Declaring metric variables
2024-12-14 16:22:11,819:INFO:Importing untrained model
2024-12-14 16:22:11,825:INFO:Logistic Regression Imported successfully
2024-12-14 16:22:11,825:INFO:Starting cross validation
2024-12-14 16:22:11,825:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-12-14 16:22:36,986:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-12-14 16:22:37,646:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-12-14 16:22:37,815:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-12-14 16:22:38,275:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-12-14 16:27:08,886:INFO:PyCaret ClassificationExperiment
2024-12-14 16:27:08,886:INFO:Logging name: clf-default-name
2024-12-14 16:27:08,886:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-12-14 16:27:08,886:INFO:version 3.3.2
2024-12-14 16:27:08,886:INFO:Initializing setup()
2024-12-14 16:27:08,886:INFO:self.USI: d342
2024-12-14 16:27:08,886:INFO:self._variable_keys: {'memory', 'USI', 'gpu_param', 'X_test', 'exp_id', 'X_train', 'exp_name_log', 'data', 'y_train', 'pipeline', 'target_param', 'seed', '_ml_usecase', 'fold_generator', 'idx', 'is_multiclass', 'fold_groups_param', '_available_plots', 'html_param', 'X', 'fold_shuffle_param', 'logging_param', 'gpu_n_jobs_param', 'n_jobs_param', 'y', 'log_plots_param', 'y_test', 'fix_imbalance'}
2024-12-14 16:27:08,886:INFO:Checking environment
2024-12-14 16:27:08,886:INFO:python_version: 3.11.8
2024-12-14 16:27:08,886:INFO:python_build: ('tags/v3.11.8:db85d51', 'Feb  6 2024 22:03:32')
2024-12-14 16:27:08,886:INFO:machine: AMD64
2024-12-14 16:27:08,886:INFO:platform: Windows-10-10.0.22631-SP0
2024-12-14 16:27:08,897:INFO:Memory: svmem(total=16890978304, available=7992102912, percent=52.7, used=8898875392, free=7992102912)
2024-12-14 16:27:08,897:INFO:Physical Core: 14
2024-12-14 16:27:08,897:INFO:Logical Core: 20
2024-12-14 16:27:08,897:INFO:Checking libraries
2024-12-14 16:27:08,897:INFO:System:
2024-12-14 16:27:08,897:INFO:    python: 3.11.8 (tags/v3.11.8:db85d51, Feb  6 2024, 22:03:32) [MSC v.1937 64 bit (AMD64)]
2024-12-14 16:27:08,897:INFO:executable: c:\Program Files\Python311\python.exe
2024-12-14 16:27:08,897:INFO:   machine: Windows-10-10.0.22631-SP0
2024-12-14 16:27:08,897:INFO:PyCaret required dependencies:
2024-12-14 16:27:08,897:INFO:                 pip: 24.0
2024-12-14 16:27:08,897:INFO:          setuptools: 65.5.0
2024-12-14 16:27:08,897:INFO:             pycaret: 3.3.2
2024-12-14 16:27:08,897:INFO:             IPython: 8.28.0
2024-12-14 16:27:08,897:INFO:          ipywidgets: 8.1.5
2024-12-14 16:27:08,897:INFO:                tqdm: 4.67.1
2024-12-14 16:27:08,897:INFO:               numpy: 1.26.4
2024-12-14 16:27:08,897:INFO:              pandas: 2.1.4
2024-12-14 16:27:08,897:INFO:              jinja2: 3.1.4
2024-12-14 16:27:08,898:INFO:               scipy: 1.11.4
2024-12-14 16:27:08,898:INFO:              joblib: 1.3.2
2024-12-14 16:27:08,898:INFO:             sklearn: 1.4.2
2024-12-14 16:27:08,898:INFO:                pyod: 2.0.2
2024-12-14 16:27:08,898:INFO:            imblearn: 0.12.4
2024-12-14 16:27:08,898:INFO:   category_encoders: 2.6.4
2024-12-14 16:27:08,898:INFO:            lightgbm: 4.5.0
2024-12-14 16:27:08,898:INFO:               numba: 0.60.0
2024-12-14 16:27:08,898:INFO:            requests: 2.32.3
2024-12-14 16:27:08,898:INFO:          matplotlib: 3.7.5
2024-12-14 16:27:08,898:INFO:          scikitplot: 0.3.7
2024-12-14 16:27:08,898:INFO:         yellowbrick: 1.5
2024-12-14 16:27:08,898:INFO:              plotly: 5.24.1
2024-12-14 16:27:08,898:INFO:    plotly-resampler: Not installed
2024-12-14 16:27:08,898:INFO:             kaleido: 0.2.1
2024-12-14 16:27:08,898:INFO:           schemdraw: 0.15
2024-12-14 16:27:08,898:INFO:         statsmodels: 0.14.4
2024-12-14 16:27:08,898:INFO:              sktime: 0.26.0
2024-12-14 16:27:08,898:INFO:               tbats: 1.1.3
2024-12-14 16:27:08,898:INFO:            pmdarima: 2.0.4
2024-12-14 16:27:08,898:INFO:              psutil: 6.1.0
2024-12-14 16:27:08,898:INFO:          markupsafe: 3.0.2
2024-12-14 16:27:08,898:INFO:             pickle5: Not installed
2024-12-14 16:27:08,898:INFO:         cloudpickle: 3.1.0
2024-12-14 16:27:08,898:INFO:         deprecation: 2.1.0
2024-12-14 16:27:08,898:INFO:              xxhash: 3.5.0
2024-12-14 16:27:08,898:INFO:           wurlitzer: Not installed
2024-12-14 16:27:08,898:INFO:PyCaret optional dependencies:
2024-12-14 16:27:08,898:INFO:                shap: Not installed
2024-12-14 16:27:08,898:INFO:           interpret: Not installed
2024-12-14 16:27:08,898:INFO:                umap: Not installed
2024-12-14 16:27:08,898:INFO:     ydata_profiling: Not installed
2024-12-14 16:27:08,898:INFO:  explainerdashboard: Not installed
2024-12-14 16:27:08,898:INFO:             autoviz: Not installed
2024-12-14 16:27:08,898:INFO:           fairlearn: Not installed
2024-12-14 16:27:08,898:INFO:          deepchecks: Not installed
2024-12-14 16:27:08,898:INFO:             xgboost: Not installed
2024-12-14 16:27:08,898:INFO:            catboost: Not installed
2024-12-14 16:27:08,898:INFO:              kmodes: Not installed
2024-12-14 16:27:08,898:INFO:             mlxtend: Not installed
2024-12-14 16:27:08,898:INFO:       statsforecast: Not installed
2024-12-14 16:27:08,898:INFO:        tune_sklearn: Not installed
2024-12-14 16:27:08,898:INFO:                 ray: Not installed
2024-12-14 16:27:08,898:INFO:            hyperopt: Not installed
2024-12-14 16:27:08,898:INFO:              optuna: Not installed
2024-12-14 16:27:08,898:INFO:               skopt: Not installed
2024-12-14 16:27:08,898:INFO:              mlflow: Not installed
2024-12-14 16:27:08,898:INFO:              gradio: Not installed
2024-12-14 16:27:08,898:INFO:             fastapi: Not installed
2024-12-14 16:27:08,898:INFO:             uvicorn: Not installed
2024-12-14 16:27:08,898:INFO:              m2cgen: Not installed
2024-12-14 16:27:08,898:INFO:           evidently: Not installed
2024-12-14 16:27:08,898:INFO:               fugue: Not installed
2024-12-14 16:27:08,898:INFO:           streamlit: Not installed
2024-12-14 16:27:08,898:INFO:             prophet: Not installed
2024-12-14 16:27:08,898:INFO:None
2024-12-14 16:27:08,898:INFO:Set up data.
2024-12-14 16:27:09,638:INFO:Set up folding strategy.
2024-12-14 16:27:09,638:INFO:Set up train/test split.
2024-12-14 16:27:10,686:INFO:Set up index.
2024-12-14 16:27:10,729:INFO:Assigning column types.
2024-12-14 16:27:11,634:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-12-14 16:27:11,656:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-12-14 16:27:11,656:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-12-14 16:27:11,672:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-12-14 16:27:11,672:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-12-14 16:27:11,706:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-12-14 16:27:11,706:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-12-14 16:27:11,722:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-12-14 16:27:11,722:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-12-14 16:27:11,722:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-12-14 16:27:11,747:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-12-14 16:27:11,765:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-12-14 16:27:11,765:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-12-14 16:27:11,785:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-12-14 16:27:11,809:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-12-14 16:27:11,809:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-12-14 16:27:11,809:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-12-14 16:27:11,847:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-12-14 16:27:11,847:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-12-14 16:27:11,889:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-12-14 16:27:11,889:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-12-14 16:27:11,889:INFO:Preparing preprocessing pipeline...
2024-12-14 16:27:12,029:INFO:Set up simple imputation.
2024-12-14 16:27:12,029:INFO:Set up feature normalization.
2024-12-14 16:27:12,152:INFO:Set up column name cleaning.
2024-12-14 16:27:15,481:INFO:Finished creating preprocessing pipeline.
2024-12-14 16:27:15,481:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\lmaos\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Unnamed: 0.1', 'Unnamed: 0',
                                             'uid', 'originh', 'originp',
                                             'responh', 'responp',
                                             'flow_duration', 'fwd_pkts_tot',
                                             'bwd_pkts_tot',
                                             'fwd_data_pkts_tot',
                                             'bwd_data_pkts_tot',
                                             'fwd_pkts_per_sec',
                                             'bwd_pkts_per_sec',
                                             'flow_p...
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-12-14 16:27:15,481:INFO:Creating final display dataframe.
2024-12-14 16:27:21,474:INFO:Setup _display_container:                     Description             Value
0                    Session id                42
1                        Target  traffic_category
2                   Target type        Multiclass
3           Original data shape      (833834, 88)
4        Transformed data shape      (833834, 88)
5   Transformed train set shape      (583683, 88)
6    Transformed test set shape      (250151, 88)
7              Numeric features                87
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                    Normalize              True
13             Normalize method            zscore
14               Fold Generator   StratifiedKFold
15                  Fold Number                10
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              d342
2024-12-14 16:27:21,513:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-12-14 16:27:21,513:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-12-14 16:27:21,547:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-12-14 16:27:21,547:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-12-14 16:27:21,557:INFO:setup() successfully completed in 12.67s...............
2024-12-14 16:27:21,557:INFO:Initializing compare_models()
2024-12-14 16:27:21,557:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002C923FC6410>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000002C923FC6410>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-12-14 16:27:21,557:INFO:Checking exceptions
2024-12-14 16:27:22,162:INFO:Preparing display monitor
2024-12-14 16:27:22,190:INFO:Initializing Logistic Regression
2024-12-14 16:27:22,190:INFO:Total runtime is 0.0 minutes
2024-12-14 16:27:22,195:INFO:SubProcess create_model() called ==================================
2024-12-14 16:27:22,195:INFO:Initializing create_model()
2024-12-14 16:27:22,195:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002C923FC6410>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002C950022ED0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-12-14 16:27:22,195:INFO:Checking exceptions
2024-12-14 16:27:22,195:INFO:Importing libraries
2024-12-14 16:27:22,195:INFO:Copying training dataset
2024-12-14 16:27:23,170:INFO:Defining folds
2024-12-14 16:27:23,170:INFO:Declaring metric variables
2024-12-14 16:27:23,171:INFO:Importing untrained model
2024-12-14 16:27:23,175:INFO:Logistic Regression Imported successfully
2024-12-14 16:27:23,175:INFO:Starting cross validation
2024-12-14 16:27:23,180:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-12-14 16:28:14,761:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-12-14 16:28:15,804:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-12-14 16:28:16,153:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-12-14 16:28:16,245:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-12-14 16:28:16,302:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-12-14 16:28:16,445:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-12-14 16:28:16,528:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-12-14 16:28:16,807:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-12-14 16:28:17,489:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-12-14 16:28:17,842:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-12-14 16:28:18,093:INFO:Calculating mean and std
2024-12-14 16:28:18,110:INFO:Creating metrics dataframe
2024-12-14 16:28:18,145:INFO:Uploading results into container
2024-12-14 16:28:18,148:INFO:Uploading model into container now
2024-12-14 16:28:18,148:INFO:_master_model_container: 1
2024-12-14 16:28:18,148:INFO:_display_container: 2
2024-12-14 16:28:18,148:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-12-14 16:28:18,148:INFO:create_model() successfully completed......................................
2024-12-14 16:28:18,610:INFO:SubProcess create_model() end ==================================
2024-12-14 16:28:18,610:INFO:Creating metrics dataframe
2024-12-14 16:28:18,618:INFO:Initializing K Neighbors Classifier
2024-12-14 16:28:18,618:INFO:Total runtime is 0.9404698014259338 minutes
2024-12-14 16:28:18,618:INFO:SubProcess create_model() called ==================================
2024-12-14 16:28:18,618:INFO:Initializing create_model()
2024-12-14 16:28:18,618:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002C923FC6410>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002C950022ED0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-12-14 16:28:18,618:INFO:Checking exceptions
2024-12-14 16:28:18,618:INFO:Importing libraries
2024-12-14 16:28:18,618:INFO:Copying training dataset
2024-12-14 16:28:19,945:INFO:Defining folds
2024-12-14 16:28:19,945:INFO:Declaring metric variables
2024-12-14 16:28:19,945:INFO:Importing untrained model
2024-12-14 16:28:19,956:INFO:K Neighbors Classifier Imported successfully
2024-12-14 16:28:19,958:INFO:Starting cross validation
2024-12-14 16:28:19,958:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-12-14 16:38:35,238:INFO:Calculating mean and std
2024-12-14 16:38:35,252:INFO:Creating metrics dataframe
2024-12-14 16:38:35,280:INFO:Uploading results into container
2024-12-14 16:38:35,284:INFO:Uploading model into container now
2024-12-14 16:38:35,290:INFO:_master_model_container: 2
2024-12-14 16:38:35,290:INFO:_display_container: 2
2024-12-14 16:38:35,293:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-12-14 16:38:35,293:INFO:create_model() successfully completed......................................
2024-12-14 16:38:35,564:INFO:SubProcess create_model() end ==================================
2024-12-14 16:38:35,564:INFO:Creating metrics dataframe
2024-12-14 16:38:35,570:INFO:Initializing Naive Bayes
2024-12-14 16:38:35,577:INFO:Total runtime is 11.223107381661732 minutes
2024-12-14 16:38:35,577:INFO:SubProcess create_model() called ==================================
2024-12-14 16:38:35,577:INFO:Initializing create_model()
2024-12-14 16:38:35,577:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002C923FC6410>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002C950022ED0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-12-14 16:38:35,577:INFO:Checking exceptions
2024-12-14 16:38:35,577:INFO:Importing libraries
2024-12-14 16:38:35,577:INFO:Copying training dataset
2024-12-14 16:38:36,826:INFO:Defining folds
2024-12-14 16:38:36,826:INFO:Declaring metric variables
2024-12-14 16:38:36,838:INFO:Importing untrained model
2024-12-14 16:38:36,838:INFO:Naive Bayes Imported successfully
2024-12-14 16:38:36,844:INFO:Starting cross validation
2024-12-14 16:38:36,845:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-12-14 16:38:57,503:INFO:Calculating mean and std
2024-12-14 16:38:57,540:INFO:Creating metrics dataframe
2024-12-14 16:38:57,582:INFO:Uploading results into container
2024-12-14 16:38:57,589:INFO:Uploading model into container now
2024-12-14 16:38:57,594:INFO:_master_model_container: 3
2024-12-14 16:38:57,594:INFO:_display_container: 2
2024-12-14 16:38:57,594:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-12-14 16:38:57,594:INFO:create_model() successfully completed......................................
2024-12-14 16:38:57,958:INFO:SubProcess create_model() end ==================================
2024-12-14 16:38:57,958:INFO:Creating metrics dataframe
2024-12-14 16:38:57,964:INFO:Initializing Decision Tree Classifier
2024-12-14 16:38:57,964:INFO:Total runtime is 11.596231806278228 minutes
2024-12-14 16:38:57,970:INFO:SubProcess create_model() called ==================================
2024-12-14 16:38:57,970:INFO:Initializing create_model()
2024-12-14 16:38:57,970:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002C923FC6410>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002C950022ED0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-12-14 16:38:57,970:INFO:Checking exceptions
2024-12-14 16:38:57,970:INFO:Importing libraries
2024-12-14 16:38:57,970:INFO:Copying training dataset
2024-12-14 16:38:58,964:INFO:Defining folds
2024-12-14 16:38:58,964:INFO:Declaring metric variables
2024-12-14 16:38:58,968:INFO:Importing untrained model
2024-12-14 16:38:58,970:INFO:Decision Tree Classifier Imported successfully
2024-12-14 16:38:58,972:INFO:Starting cross validation
2024-12-14 16:38:58,972:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-12-14 16:39:34,155:INFO:Calculating mean and std
2024-12-14 16:39:34,182:INFO:Creating metrics dataframe
2024-12-14 16:39:34,193:INFO:Uploading results into container
2024-12-14 16:39:34,211:INFO:Uploading model into container now
2024-12-14 16:39:34,211:INFO:_master_model_container: 4
2024-12-14 16:39:34,211:INFO:_display_container: 2
2024-12-14 16:39:34,211:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2024-12-14 16:39:34,211:INFO:create_model() successfully completed......................................
2024-12-14 16:39:34,503:INFO:SubProcess create_model() end ==================================
2024-12-14 16:39:34,503:INFO:Creating metrics dataframe
2024-12-14 16:39:34,509:INFO:Initializing SVM - Linear Kernel
2024-12-14 16:39:34,509:INFO:Total runtime is 12.205309331417082 minutes
2024-12-14 16:39:34,519:INFO:SubProcess create_model() called ==================================
2024-12-14 16:39:34,519:INFO:Initializing create_model()
2024-12-14 16:39:34,519:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002C923FC6410>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002C950022ED0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-12-14 16:39:34,519:INFO:Checking exceptions
2024-12-14 16:39:34,519:INFO:Importing libraries
2024-12-14 16:39:34,519:INFO:Copying training dataset
2024-12-14 16:39:35,756:INFO:Defining folds
2024-12-14 16:39:35,756:INFO:Declaring metric variables
2024-12-14 16:39:35,764:INFO:Importing untrained model
2024-12-14 16:39:35,765:INFO:SVM - Linear Kernel Imported successfully
2024-12-14 16:39:35,770:INFO:Starting cross validation
2024-12-14 16:39:35,771:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-12-14 16:40:05,948:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-12-14 16:40:08,365:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-12-14 16:40:09,050:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-12-14 16:40:09,438:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-12-14 16:40:10,415:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-12-14 16:40:10,657:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-12-14 16:40:10,955:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-12-14 16:40:10,965:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-12-14 16:40:11,335:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-12-14 16:40:11,442:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-12-14 16:40:11,671:INFO:Calculating mean and std
2024-12-14 16:40:11,706:INFO:Creating metrics dataframe
2024-12-14 16:40:11,722:INFO:Uploading results into container
2024-12-14 16:40:11,722:INFO:Uploading model into container now
2024-12-14 16:40:11,722:INFO:_master_model_container: 5
2024-12-14 16:40:11,722:INFO:_display_container: 2
2024-12-14 16:40:11,737:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=42, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-12-14 16:40:11,737:INFO:create_model() successfully completed......................................
2024-12-14 16:40:12,042:INFO:SubProcess create_model() end ==================================
2024-12-14 16:40:12,042:INFO:Creating metrics dataframe
2024-12-14 16:40:12,059:INFO:Initializing Ridge Classifier
2024-12-14 16:40:12,059:INFO:Total runtime is 12.831151914596555 minutes
2024-12-14 16:40:12,061:INFO:SubProcess create_model() called ==================================
2024-12-14 16:40:12,061:INFO:Initializing create_model()
2024-12-14 16:40:12,061:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002C923FC6410>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002C950022ED0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-12-14 16:40:12,061:INFO:Checking exceptions
2024-12-14 16:40:12,061:INFO:Importing libraries
2024-12-14 16:40:12,061:INFO:Copying training dataset
2024-12-14 16:40:13,370:INFO:Defining folds
2024-12-14 16:40:13,370:INFO:Declaring metric variables
2024-12-14 16:40:13,373:INFO:Importing untrained model
2024-12-14 16:40:13,375:INFO:Ridge Classifier Imported successfully
2024-12-14 16:40:13,375:INFO:Starting cross validation
2024-12-14 16:40:13,375:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-12-14 16:40:33,691:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-12-14 16:40:33,706:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-12-14 16:40:33,801:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-12-14 16:40:33,897:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-12-14 16:40:34,077:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-12-14 16:40:34,100:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-12-14 16:40:34,143:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-12-14 16:40:34,143:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-12-14 16:40:34,184:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-12-14 16:40:34,206:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-12-14 16:40:34,439:INFO:Calculating mean and std
2024-12-14 16:40:34,467:INFO:Creating metrics dataframe
2024-12-14 16:40:34,492:INFO:Uploading results into container
2024-12-14 16:40:34,495:INFO:Uploading model into container now
2024-12-14 16:40:34,495:INFO:_master_model_container: 6
2024-12-14 16:40:34,495:INFO:_display_container: 2
2024-12-14 16:40:34,495:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2024-12-14 16:40:34,495:INFO:create_model() successfully completed......................................
2024-12-14 16:40:34,798:INFO:SubProcess create_model() end ==================================
2024-12-14 16:40:34,798:INFO:Creating metrics dataframe
2024-12-14 16:40:34,811:INFO:Initializing Random Forest Classifier
2024-12-14 16:40:34,811:INFO:Total runtime is 13.210338544845579 minutes
2024-12-14 16:40:34,811:INFO:SubProcess create_model() called ==================================
2024-12-14 16:40:34,811:INFO:Initializing create_model()
2024-12-14 16:40:34,811:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002C923FC6410>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002C950022ED0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-12-14 16:40:34,811:INFO:Checking exceptions
2024-12-14 16:40:34,811:INFO:Importing libraries
2024-12-14 16:40:34,811:INFO:Copying training dataset
2024-12-14 16:40:35,946:INFO:Defining folds
2024-12-14 16:40:35,946:INFO:Declaring metric variables
2024-12-14 16:40:35,953:INFO:Importing untrained model
2024-12-14 16:40:35,956:INFO:Random Forest Classifier Imported successfully
2024-12-14 16:40:35,957:INFO:Starting cross validation
2024-12-14 16:40:35,962:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-12-14 16:43:25,064:INFO:Calculating mean and std
2024-12-14 16:43:25,090:INFO:Creating metrics dataframe
2024-12-14 16:43:25,113:INFO:Uploading results into container
2024-12-14 16:43:25,114:INFO:Uploading model into container now
2024-12-14 16:43:25,119:INFO:_master_model_container: 7
2024-12-14 16:43:25,119:INFO:_display_container: 2
2024-12-14 16:43:25,123:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2024-12-14 16:43:25,123:INFO:create_model() successfully completed......................................
2024-12-14 16:43:25,475:INFO:SubProcess create_model() end ==================================
2024-12-14 16:43:25,475:INFO:Creating metrics dataframe
2024-12-14 16:43:25,484:INFO:Initializing Quadratic Discriminant Analysis
2024-12-14 16:43:25,484:INFO:Total runtime is 16.054903030395504 minutes
2024-12-14 16:43:25,486:INFO:SubProcess create_model() called ==================================
2024-12-14 16:43:25,486:INFO:Initializing create_model()
2024-12-14 16:43:25,486:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002C923FC6410>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002C950022ED0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-12-14 16:43:25,486:INFO:Checking exceptions
2024-12-14 16:43:25,486:INFO:Importing libraries
2024-12-14 16:43:25,486:INFO:Copying training dataset
2024-12-14 16:43:26,779:INFO:Defining folds
2024-12-14 16:43:26,779:INFO:Declaring metric variables
2024-12-14 16:43:26,779:INFO:Importing untrained model
2024-12-14 16:43:26,791:INFO:Quadratic Discriminant Analysis Imported successfully
2024-12-14 16:43:26,795:INFO:Starting cross validation
2024-12-14 16:43:26,796:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-12-14 16:43:49,754:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-12-14 16:43:49,839:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-12-14 16:43:49,990:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-12-14 16:43:50,427:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-12-14 16:43:50,501:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-12-14 16:43:50,592:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-12-14 16:43:50,639:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-12-14 16:43:50,674:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-12-14 16:43:50,720:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-12-14 16:44:13,461:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-12-14 16:44:13,478:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-12-14 16:44:13,529:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-12-14 16:44:13,551:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-12-14 16:44:13,680:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-12-14 16:44:13,727:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-12-14 16:44:13,825:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-12-14 16:44:13,913:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-12-14 16:44:13,949:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-12-14 16:44:14,003:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-12-14 16:44:14,233:INFO:Calculating mean and std
2024-12-14 16:44:14,256:INFO:Creating metrics dataframe
2024-12-14 16:44:14,273:INFO:Uploading results into container
2024-12-14 16:44:14,273:INFO:Uploading model into container now
2024-12-14 16:44:14,287:INFO:_master_model_container: 8
2024-12-14 16:44:14,287:INFO:_display_container: 2
2024-12-14 16:44:14,287:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-12-14 16:44:14,287:INFO:create_model() successfully completed......................................
2024-12-14 16:44:14,654:INFO:SubProcess create_model() end ==================================
2024-12-14 16:44:14,654:INFO:Creating metrics dataframe
2024-12-14 16:44:14,667:INFO:Initializing Ada Boost Classifier
2024-12-14 16:44:14,667:INFO:Total runtime is 16.874618200461065 minutes
2024-12-14 16:44:14,667:INFO:SubProcess create_model() called ==================================
2024-12-14 16:44:14,667:INFO:Initializing create_model()
2024-12-14 16:44:14,667:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002C923FC6410>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002C950022ED0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-12-14 16:44:14,667:INFO:Checking exceptions
2024-12-14 16:44:14,667:INFO:Importing libraries
2024-12-14 16:44:14,667:INFO:Copying training dataset
2024-12-14 16:44:15,766:INFO:Defining folds
2024-12-14 16:44:15,766:INFO:Declaring metric variables
2024-12-14 16:44:15,766:INFO:Importing untrained model
2024-12-14 16:44:15,771:INFO:Ada Boost Classifier Imported successfully
2024-12-14 16:44:15,773:INFO:Starting cross validation
2024-12-14 16:44:15,776:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-12-14 16:44:33,878:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-12-14 16:44:33,878:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-12-14 16:44:33,878:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-12-14 16:44:34,088:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-12-14 16:44:34,092:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-12-14 16:44:34,186:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-12-14 16:44:34,227:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-12-14 16:44:34,402:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-12-14 16:44:34,479:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-12-14 16:44:34,510:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-12-14 16:48:11,258:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-12-14 16:48:11,340:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-12-14 16:48:11,568:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-12-14 16:48:11,625:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-12-14 16:48:12,342:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-12-14 16:48:12,378:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-12-14 16:48:13,712:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-12-14 16:48:13,754:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-12-14 16:48:13,790:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-12-14 16:48:13,830:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-12-14 16:48:14,258:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-12-14 16:48:14,289:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-12-14 16:48:14,439:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-12-14 16:48:14,465:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-12-14 16:48:14,475:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-12-14 16:48:14,506:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-12-14 16:48:14,516:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-12-14 16:48:14,550:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-12-14 16:48:15,168:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-12-14 16:48:15,195:WARNING:C:\Users\lmaos\AppData\Roaming\Python\Python311\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-12-14 16:48:15,376:INFO:Calculating mean and std
2024-12-14 16:48:15,384:INFO:Creating metrics dataframe
2024-12-14 16:48:15,410:INFO:Uploading results into container
2024-12-14 16:48:15,420:INFO:Uploading model into container now
2024-12-14 16:48:15,422:INFO:_master_model_container: 9
2024-12-14 16:48:15,422:INFO:_display_container: 2
2024-12-14 16:48:15,422:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=42)
2024-12-14 16:48:15,422:INFO:create_model() successfully completed......................................
2024-12-14 16:48:15,705:INFO:SubProcess create_model() end ==================================
2024-12-14 16:48:15,705:INFO:Creating metrics dataframe
2024-12-14 16:48:15,718:INFO:Initializing Gradient Boosting Classifier
2024-12-14 16:48:15,718:INFO:Total runtime is 20.89213694731394 minutes
2024-12-14 16:48:15,723:INFO:SubProcess create_model() called ==================================
2024-12-14 16:48:15,723:INFO:Initializing create_model()
2024-12-14 16:48:15,723:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002C923FC6410>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002C950022ED0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-12-14 16:48:15,723:INFO:Checking exceptions
2024-12-14 16:48:15,723:INFO:Importing libraries
2024-12-14 16:48:15,724:INFO:Copying training dataset
2024-12-14 16:48:16,820:INFO:Defining folds
2024-12-14 16:48:16,820:INFO:Declaring metric variables
2024-12-14 16:48:16,820:INFO:Importing untrained model
2024-12-14 16:48:16,828:INFO:Gradient Boosting Classifier Imported successfully
2024-12-14 16:48:16,833:INFO:Starting cross validation
2024-12-14 16:48:16,833:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-12-14 18:24:37,704:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-12-14 18:24:37,704:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-12-14 18:24:37,704:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-12-14 18:24:37,704:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-12-14 19:22:02,339:WARNING:C:\Users\lmaos\AppData\Local\Temp\ipykernel_21632\2018786256.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 0.1),

2024-12-14 19:22:02,339:WARNING:C:\Users\lmaos\AppData\Local\Temp\ipykernel_21632\2018786256.py:7: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),

2024-12-14 19:22:02,344:WARNING:C:\Users\lmaos\AppData\Local\Temp\ipykernel_21632\2018786256.py:8: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),

2024-12-14 19:22:02,344:WARNING:C:\Users\lmaos\AppData\Local\Temp\ipykernel_21632\2018786256.py:9: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-5, 10.0),

2024-12-14 19:22:02,344:WARNING:C:\Users\lmaos\AppData\Local\Temp\ipykernel_21632\2018786256.py:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-5, 10.0),

2024-12-14 19:24:58,239:WARNING:C:\Users\lmaos\AppData\Local\Temp\ipykernel_21632\2018786256.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 0.1),

2024-12-14 19:24:58,239:WARNING:C:\Users\lmaos\AppData\Local\Temp\ipykernel_21632\2018786256.py:7: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),

2024-12-14 19:24:58,240:WARNING:C:\Users\lmaos\AppData\Local\Temp\ipykernel_21632\2018786256.py:8: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),

2024-12-14 19:24:58,240:WARNING:C:\Users\lmaos\AppData\Local\Temp\ipykernel_21632\2018786256.py:9: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-5, 10.0),

2024-12-14 19:24:58,240:WARNING:C:\Users\lmaos\AppData\Local\Temp\ipykernel_21632\2018786256.py:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-5, 10.0),

2024-12-14 19:30:29,894:WARNING:C:\Users\lmaos\AppData\Local\Temp\ipykernel_21632\2018786256.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 0.1),

2024-12-14 19:30:29,894:WARNING:C:\Users\lmaos\AppData\Local\Temp\ipykernel_21632\2018786256.py:7: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),

2024-12-14 19:30:29,894:WARNING:C:\Users\lmaos\AppData\Local\Temp\ipykernel_21632\2018786256.py:8: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),

2024-12-14 19:30:29,894:WARNING:C:\Users\lmaos\AppData\Local\Temp\ipykernel_21632\2018786256.py:9: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-5, 10.0),

2024-12-14 19:30:29,894:WARNING:C:\Users\lmaos\AppData\Local\Temp\ipykernel_21632\2018786256.py:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-5, 10.0),

2024-12-14 19:33:05,128:WARNING:C:\Users\lmaos\AppData\Local\Temp\ipykernel_21632\2018786256.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 0.1),

2024-12-14 19:33:05,128:WARNING:C:\Users\lmaos\AppData\Local\Temp\ipykernel_21632\2018786256.py:7: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),

2024-12-14 19:33:05,128:WARNING:C:\Users\lmaos\AppData\Local\Temp\ipykernel_21632\2018786256.py:8: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),

2024-12-14 19:33:05,128:WARNING:C:\Users\lmaos\AppData\Local\Temp\ipykernel_21632\2018786256.py:9: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-5, 10.0),

2024-12-14 19:33:05,128:WARNING:C:\Users\lmaos\AppData\Local\Temp\ipykernel_21632\2018786256.py:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-5, 10.0),

